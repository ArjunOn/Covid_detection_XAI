# -*- coding: utf-8 -*-
"""Efficientnet_covid_xai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xzd-B9yfxWb5sq26xQ12YEe-xsj2UwWw
"""

import os
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.utils import shuffle
import tensorflow.keras.backend as K

# ============ GPU SETUP FOR HPC ============ #
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        # Enable memory growth
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

        # Optionally limit GPU memory (e.g., 12 GB)
        tf.config.set_logical_device_configuration(
            gpus[0],
            [tf.config.LogicalDeviceConfiguration(memory_limit=12000)]
        )

        logical_gpus = tf.config.list_logical_devices('GPU')
        print(f"{len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s). Memory growth enabled.")
    except RuntimeError as e:
        print("GPU config error:", e)
else:
    print("No GPU detected. Running on CPU.")


# ============ CONFIGURATION ============
DATASET_PATH = "/mnt/home/amudalap/Arjun/COVID-19_Radiography_Dataset"
IMAGE_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 20
NUM_CLASSES = 2
MAX_IMAGES_PER_CLASS = 3616
MODEL_OUTPUT = 'efficientnet_covid_classifier.keras'
VISUALIZATION_OUTPUT_DIR = 'gradcam_outputs'
os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)

# ============ LOAD IMAGE PATHS ============
class_labels = []
for label in os.listdir(DATASET_PATH):
    class_path = os.path.join(DATASET_PATH, label)
    for img in os.listdir(class_path):
        class_labels.append((label, os.path.join(class_path, img)))
df = pd.DataFrame(class_labels, columns=['Labels', 'image'])

# ============ IMAGE PROCESSING ============
images, labels = [], []
for label in df['Labels'].unique():
    subset = df[df['Labels'] == label][:MAX_IMAGES_PER_CLASS]
    for img_path in subset['image']:
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))
            images.append(img)
            labels.append(label)

images = np.array(images).astype('float32') / 255.0 #what is normalization???

# ============ LABEL ENCODING ============
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels).reshape(-1, 1)
ct = ColumnTransformer([("onehot", OneHotEncoder(), [0])], remainder='passthrough')
Y = ct.fit_transform(encoded_labels)

# ============ DATA SPLIT ============
images, Y = shuffle(images, Y, random_state=1)
train_x, temp_x, train_y, temp_y = train_test_split(images, Y, test_size=0.2, random_state=42)
val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.5, random_state=42)

# ============ MODEL DEFINITION ============
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import callbacks

model = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# ============ DATA PIPELINE ============
train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# ============ CALLBACKS ============
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
checkpoint = callbacks.ModelCheckpoint(filepath='best_model_effnetb0.h5', monitor='val_accuracy', save_best_only=True, mode='max')

# ============ TRAINING ============
history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, verbose=2, callbacks=[early_stop, checkpoint])
model.save(MODEL_OUTPUT)

# ============ PLOT TRAINING HISTORY ============
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.legend()
plt.tight_layout()
plt.savefig('training_history.png')
plt.close()

# ============ EVALUATE ON TEST SET ============
test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(BATCH_SIZE)
test_loss, test_acc = model.evaluate(test_dataset, verbose=0)
print(f"Test Accuracy: {test_acc:.4f}, Loss: {test_loss:.4f}")

# ============ CLASSIFICATION REPORT ============
y_pred = model.predict(test_x)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(test_y, axis=1)
report = classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_)
print(report)
with open("classification_report.txt", "w") as f:
    f.write(report)

# ============ CONFUSION MATRIX ============
cm = confusion_matrix(y_true, y_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig("confusion_matrix.png")
plt.close()

# ============ GRAD-CAM FUNCTIONS ============
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_output = predictions[:, pred_index]
    grads = tape.gradient(class_output, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def superimpose_heatmap(heatmap, image, alpha=0.4):
    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))
    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)
    superimposed_img = heatmap_colored * alpha + image
    return np.uint8(superimposed_img)

# ============ GENERATE & SAVE GRAD-CAM IMAGES ============
last_conv_layer = "top_conv"
for idx in range(10):
    input_img = test_x[idx]
    img_array = np.expand_dims(input_img, axis=0)
    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer)
    input_img_rgb = (input_img * 255).astype(np.uint8)
    superimposed_img = superimpose_heatmap(heatmap, input_img_rgb)

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1)
    plt.title("Original")
    plt.imshow(input_img_rgb)
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.title("Grad-CAM Heatmap")
    plt.imshow(heatmap, cmap='jet')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.title("Superimposed")
    plt.imshow(superimposed_img)
    plt.axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(VISUALIZATION_OUTPUT_DIR, f"gradcam_sample{idx}.png"))
    plt.close()